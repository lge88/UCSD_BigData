{
 "metadata": {
  "name": "",
  "signature": "sha256:e912fb5612e16feeae7cc6218424bc8b7e384fa15c16f65513a9a3dbb6129b5d"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Evaluation"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "In this project, we employed PCA and Kolmogorov\u2013Smirnov test to explore global temperature patterns. To evaluate the performance of these two strategies, we made a gold standard of climate patterns, which we believe is consistent with the temperature patterns. \n",
      "\n",
      "<b>Background</b><br></br>\n",
      "Evaluation set\n",
      "Considering a majority of the stations are in US and very limited time, we decided only label stations and nodes in US continent, Alaska and Hawaii, and we focused on the performance of merging nodes of 9 digit node ID (i.e. the bottom level). Following this criterion, we identified 17356 stations in US. \n",
      "\n",
      "Climate patters\n",
      "We adopted this widely used climate zone map from NCDC (http://www.ncdc.noaa.gov/monitoring-references/maps/us-climate-regions.php). In this map the continental US is divided into 9 zones and we added two more Alaska and Hawaii. \n",
      "To label stations with zone names, we started from finding the boundary of every state using this site http://econym.org.uk/gmap/states.xml. \n",
      "To simplify the task, we approximated each state with a rectangle of the maximum and minnum values of latitude and longitude of that state; all the stations within this range were labeled with that state. After this there is a manual check step to remove those incorrectly classified stations with help from an interative map http://itouchmap.com/latlong.html. This manual check is still incomplete but a part of stations have been corrected. Checked boundaries are stored at /home/ubuntu/UCSD_BigData/data/weather/boundary.pkl. \n",
      "\n",
      "Comparison of PCA results with gold standard and criteria\n",
      "PCA merges stations/nodes based on miminum description length(MDL). We found 200 pairs across all levels and 120 pairs at the bottom level. If a pair of merged stations/nodes were from the same climate zone, it is a true positive result; if a pair of merged stations/nodes were from different climate zone, it is a false positive; if two stations/nodes were from the same zone but not merged, it is a false negative result; finally, it two stations/nodes were from different zones and were not merged, if is a true negative result. \n",
      "\n",
      "Kolmogorov\u2013Smirnov test based merge\n",
      "To find temparature trends over the years we calculated the annual average TMAX and TMIN over the world. This results may be helpful for extreme temperature review. Actually we found similar results from http://www.ncdc.noaa.gov/extremes/. "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<b>Results</b><br></br>\n",
      "<b>1. PCA result evaluation</b><br></br>"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# We started from Professor's prebuilt tree. \n",
      "# this part of code get a pandas dataframe of partitioned stations, with node ID\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import pickle\n",
      "import re\n",
      "# get all the stations\n",
      "# tree.pkl is available from s3://weiwei.bucket/data/tree.pkl\n",
      "treeDir = \"/home/ubuntu/UCSD_BigData/data/weather/tree.pkl\" \n",
      "tree = pickle.load(file(treeDir)) \n",
      "stns = tree['Partitioned_Stations']  # Partitioned_Stations"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 3
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Then we mark boundaries of states."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# This section of code generates the approximated boundaries of states\n",
      "import re\n",
      "import pickle\n",
      "fin = file(\"/home/ubuntu/UCSD_BigData/data/weather/usmapcoord.txt\") # this is the boundary data mentioned in the first section.\n",
      "lines = fin.readlines()\n",
      "states=[]\n",
      "res={}\n",
      "lat=[]\n",
      "lon=[]\n",
      "for ln in lines:\n",
      "    if ln.startswith(\"<state name\"): #begining of a state\n",
      "        stateName = re.findall('name=\"([a-z A-Z]*)\" colour',ln)[0]\n",
      "        res[stateName]=[]\n",
      "    elif ln.startswith('</state>'): # end of a state\n",
      "        res[stateName]=[max(lat),min(lat),max(lon),min(lon)]\n",
      "        lat=[]\n",
      "        lon=[]        \n",
      "    else:\n",
      "        lalo=re.findall('\"([-.\\d]*)\"?',ln)\n",
      "        lalo=[float(x) for x in lalo]\n",
      "        lat.append(lalo[0])\n",
      "        lon.append(lalo[1])\n",
      "\n",
      "print res\n",
      "fout=file(\"/home/ubuntu/UCSD_BigData/data/weather/boundary.pkl\",\"w\") \n",
      "pickle.dump(res,fout)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "{'Mississippi': [35.0075, 30.0905, -88.0994, -91.6589], 'Oklahoma': [37.0015, 33.6386, -94.4357, -103.0064], 'Delaware': [39.8296, 38.4482, -74.8526, -75.7919], 'Minnesota': [49.3877, 43.5008, -89.4919, -97.2304], 'Illinois': [42.5116, 36.9894, -87.0213, -91.512], 'Arkansas': [36.4997, 33.0075, -89.6594, -94.6198], 'New Mexico': [36.9982, 31.3337, -103.0023, -109.0489], 'Indiana': [41.7611, 37.7718, -84.809, -88.098], 'Maryland': [39.722, 37.8889, -74.8581, -79.4861], 'Louisiana': [33.0225, 28.8832, -88.7421, -94.043], 'Idaho': [49.0018, 41.9871, -111.0471, -117.2372], 'Wyoming': [44.9998, 40.9986, -104.0556, -111.0539], 'Tennessee': [36.6871, 34.9884, -81.6518, -90.3131], 'Arizona': [37.0004, 31.3325, -109.0475, -114.8126], 'Iowa': [43.5008, 40.3622, -90.1538, -96.6357], 'Michigan': [48.3042, 41.6965, -82.1221, -90.4175], 'Kansas': [40.0087, 36.9927, -94.6046, -102.0506], 'Utah': [41.9993, 36.9982, -109.0462, -114.0504], 'Virginia': [39.4659, 36.5427, -74.9707, -83.6753], 'Oregon': [46.2891, 41.9952, -116.4606, -124.7305], 'Connecticut': [42.0511, 40.9509, -71.7874, -73.7272], 'Montana': [48.9991, 44.3563, -104.0186, -116.0458], 'California': [42.0126, 32.5121, -114.1315, -124.6509], 'Massachusetts': [42.889, 41.159, -69.7398, -73.5081], 'West Virginia': [40.6338, 37.1953, -77.731, -82.6392], 'South Carolina': [35.2075, 32.0453, -78.4836, -83.3588], 'New Hampshire': [45.3058, 42.6986, -70.5583, -72.5592], 'Wisconsin': [47.31, 42.4954, -86.2523, -92.8564], 'Vermont': [45.0153, 42.7289, -71.4949, -73.4381], 'Georgia': [34.9996, 30.3575, -80.696, -85.6082], 'North Dakota': [48.9982, 45.934, -96.5671, -104.0501], 'Pennsylvania': [42.5167, 39.7199, -74.707, -80.5243], 'Florida': [31.0035, 24.3959, -79.8198, -87.6256], 'Alaska': [71.5232, 52.5964, -129.993, -169.9146], 'Kentucky': [39.1439, 36.4931, -82.0308, -89.5372], 'Hawaii': [22.3386, 18.71, -154.6271, -160.3922], 'Nebraska': [43.0006, 39.9992, -95.3091, -104.0543], 'Missouri': [40.6181, 35.9958, -89.1005, -95.7527], 'Ohio': [42.321, 38.3761, -80.5188, -84.8172], 'Alabama': [35.0041, 30.1463, -84.8927, -88.4743], 'Rhode Island': [42.0156, 41.1849, -71.0541, -71.9041], 'South Dakota': [45.9435, 42.4772, -96.438, -104.0529], 'Colorado': [41.0006, 36.9949, -102.0424, -109.0489], 'New Jersey': [41.3593, 38.8472, -73.8885, -75.5708], 'Washington': [49.0027, 45.5439, -116.9165, -124.8679], 'North Carolina': [36.588, 33.7666, -75.4129, -84.3201], 'New York': [45.0153, 40.4772, -71.7517, -79.7624], 'Texas': [36.5008, 25.8419, -93.5074, -106.6168], 'Nevada': [42.0003, 35.003, -114.0436, -120.0037], 'Maine': [47.455, 42.9182, -66.8628, -71.0829]}\n"
       ]
      }
     ],
     "prompt_number": 30
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Now define regions"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#Map climate regions and states\n",
      "import pickle\n",
      "f=file(\"/home/ubuntu/UCSD_BigData/data/weather/boundary.pkl\",'r')\n",
      "boundary = pickle.load(f)\n",
      "\n",
      "regionDict= {'Mississippi': 'South', 'Oklahoma': 'South', 'Wyoming': 'WestNorthCentral', 'Minnesota': 'EastNorthCentral',\\\n",
      "             'Illinois': 'Central', 'Arkansas': 'South', 'New Mexico': 'Southwest', 'Indiana': 'Central', 'Maryland': 'Northeast',\\\n",
      "             'Louisiana': 'South', 'Idaho': 'Northwest', 'Tennessee': 'Central', 'Arizona': 'Southwest', 'Iowa': 'EastNorthCentral',\\\n",
      "             'Michigan': 'EastNorthCentral', 'Kansas': 'South', 'Utah': 'Southwest', 'Virginia': 'Southeast', 'Oregon': 'Northwest',\\\n",
      "             'Connecticut': 'Northeast', 'Montana': 'WestNorthCentral', 'California': 'West', 'Massachusetts': 'Northeast', \\\n",
      "             'West Virginia': 'Central', 'South Carolina': 'Southeast', 'New Hampshire': 'Northeast', 'Wisconsin': 'EastNorthCentral',\\\n",
      "             'Vermont': 'Northeast', 'Georgia': 'Southeast', 'North Dakota': 'WestNorthCentral', 'Pennsylvania': 'Northeast',\\\n",
      "             'Florida': 'Southeast', 'Alaska': 'Alaska', 'Kentucky': 'Central', 'Hawaii': 'Hawaii', 'Nebraska': 'WestNorthCentral',\\\n",
      "             'Missouri': 'Central', 'Ohio': 'Central', 'Alabama': 'Southeast', 'Rhode Island': 'Northeast', 'South Dakota': 'WestNorthCentral',\\\n",
      "             'Colorado': 'Southwest', 'New Jersey': 'Northeast', 'Washington': 'Northwest', 'North Carolina': 'Southeast', \\\n",
      "             'New York': 'Northeast', 'Texas': 'South', 'Nevada': 'West', 'Delaware': 'Northeast', 'Maine': 'Northeast'}\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 31
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Next, add two columns 'st' for state and 'region' in partitioned station data frame. "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "stns['st'] = pd.Series(['st']*stns.shape[0], index=stns.index)\n",
      "stns['region'] = pd.Series(['region']*stns.shape[0], index=stns.index)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 34
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Update the state and region information for US station/nodes"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "for i in xrange(stns.shape[0]):\n",
      "    index = stns.index[i]\n",
      "    lat=stns.ix[index][\"latitude\"]\n",
      "    lon=stns.ix[index][\"longitude\"]\n",
      "    for key in boundary.keys():\n",
      "        latMax,latMin,lonMax,lonMin=boundary[key]\n",
      "        if latMin<lat<latMax and lonMin<lon<lonMax:\n",
      "            stns.loc[index,'st']=key\n",
      "            stns.loc[index,'region']=regionDict[key]\n",
      "with file(\"updateSTN\",\"w\") as f:\n",
      "    pickle.dump(stns,f)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "stns=pickle.load(file(\"updateSTN\",\"r\"))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 8
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The above results were manually modified. This is a very time consuming work so we didn't finish it. <br>\n",
      "So far, the labeled station/node dataframe is ready. Let's look into PCA results.<br>\n",
      "First, get a list of merged nodes and stations from PCA. Count the number of merged pairs. We made a subset of station nodes which have 9 digits in node ID for gold standard."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# get a list of merged pairs, stored in mdlRes; make a subset of station nodes which have 9 digits in node ID, store this in mdlTestPos\n",
      "mdl=\"/home/ubuntu/lge_bigdata/UCSD_BigData/notebooks/lge-hw2/data/node-pairs-should-merge-using-mdl.csv\" \n",
      "#  this file is available from s3://weiwei.bucket/data/node-pairs-should-merge-using-mdl.csv\n",
      "mdlRes=file(mdl).read().split(\"\\n\")\n",
      "mdlRes=[mr.split(\",\") for mr in mdlRes]\n",
      "mdlTestPos = filter(lambda x:len(x[0])==9,mdlRes) # only keep station nodes\n",
      "print \"Num of test positive \",len(mdlTestPos)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Num of test positive  120\n"
       ]
      }
     ],
     "prompt_number": 10
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Then, count the number of qualified US stations.17356 stations were found in 50 states."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# count qualified US stations\n",
      "stnCount=0\n",
      "for state in boundary.keys():\n",
      "    stnCount+=stns[stns['st']==state].shape[0]\n",
      "print stnCount"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "17356\n"
       ]
      }
     ],
     "prompt_number": 11
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Get count of unique nodes in every region.We assume all nodes from the same region should be merged. From the counts we can estimate the number of merged pairs, named condition positive pairs. We got 11825 pairs."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Num of pairs according to gold standard\n",
      "regionList = list(set(regionDict.values()))\n",
      "allPairs = 0\n",
      "\n",
      "for region in regionList:\n",
      "    area = stns[stns['region']==region]\n",
      "    uniqueNodes = list(set(area['Node']))\n",
      "    print region,len(uniqueNodes)\n",
      "    numPair = len(uniqueNodes)*(len(uniqueNodes)-1)/2\n",
      "    allPairs+=numPair\n",
      "print \"num of condition positive pairs \",allPairs"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Northeast 46\n",
        "Central 66\n",
        "West 36\n",
        "Hawaii 4\n",
        "Alaska 11\n",
        "EastNorthCentral 47\n",
        "Southeast 44\n",
        "Northwest 38\n",
        "WestNorthCentral 65\n",
        "South 64\n",
        "Southwest"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 48\n",
        "num of condition positive pairs  11825\n"
       ]
      }
     ],
     "prompt_number": 12
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We also need a complete list of all possible pairs from gold standard, stored in allPosPairList. The result below shows regions and corresponding pair numbers."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# All positive pairs according to gold standard\n",
      "regionList = list(set(regionDict.values()))\n",
      "allPosPairList=[]\n",
      "for region in regionList:\n",
      "    regionPair=[]\n",
      "    area = stns[stns['region']==region]\n",
      "    uniqueNodes = list(set(area['Node']))\n",
      "    for i in range(len(uniqueNodes)):\n",
      "        for j in range(i+1,len(uniqueNodes)):\n",
      "            regionPair.append((uniqueNodes[i],uniqueNodes[j]))\n",
      "    print region,len(regionPair)\n",
      "    allPosPairList+=regionPair"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Northeast 1035\n",
        "Central 2145\n",
        "West 630\n",
        "Hawaii 6\n",
        "Alaska 55\n",
        "EastNorthCentral 1081\n",
        "Southeast 946\n",
        "Northwest 703\n",
        "WestNorthCentral 2080\n",
        "South"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 2016\n",
        "Southwest 1128\n"
       ]
      }
     ],
     "prompt_number": 14
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Compare PCA output and labeled map, we get true positive merges. There are 47 pairs among 120.\n",
      "As explained in the first section, if a pair of merged stations/nodes were from the same climate region, it is true positive; if a pair of merged stations/nodes were from different climate regions, it is false positive; if two stations/nodes were from the same region but not merged, it is false negative; finally, if two stations/nodes were from different zones and were not merged, it is true negative."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# count true positive pairs \n",
      "truePosNum = 0\n",
      "for pair in mdlTestPos:\n",
      "    if tuple(pair) in allPosPairList:\n",
      "        truePosNum+=1\n",
      "print truePosNum"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "47\n"
       ]
      }
     ],
     "prompt_number": 15
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# False negative number is 11778\n",
      "11825-47"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 16,
       "text": [
        "11778"
       ]
      }
     ],
     "prompt_number": 16
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# False positive number is 73\n",
      "120-47"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 17,
       "text": [
        "73"
       ]
      }
     ],
     "prompt_number": 17
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# True negative number is 97921\n",
      "regionList = list(set(regionDict.values()))\n",
      "regionNodeNumList=[] #  a list of unique nodes in every region\n",
      "for region in regionList:\n",
      "    regionPair=[]\n",
      "    area = stns[stns['region']==region]\n",
      "    uniqueNodes = list(set(area['Node']))\n",
      "    regionNodeNumList.append(len(uniqueNodes))\n",
      "print \"num of nodes in every region \",regionNodeNumList\n",
      "# negative pairs\n",
      "negPairNum=0\n",
      "for i in range(len(regionNodeNumList)):\n",
      "    for j in range(i+1,len(regionNodeNumList)):\n",
      "        negPairNum+=regionNodeNumList[i]*regionNodeNumList[j]\n",
      "print \"negPairNum: \",negPairNum"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "num of nodes in every region  [46, 66, 36, 4, 11, 47, 44, 38, 65, 64, 48]\n",
        "negPairNum:  97921\n"
       ]
      }
     ],
     "prompt_number": 19
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Given TP=47, FP=73, FN=11778, TN=97921, we can evaluate sensitivity, sepcificity and accuracy. Apparently this is a very rough estimation, because we used a limited subset to evaluate the performance of PCA of the entire dataset; gold standard could be further improved; etc. But these values could be suggestive. \n",
      "Sensitivity = 47/(47+11778) = 0.003974630021141649\n",
      "Specificity = 97921/(97921+73) = 0.9992550564320265\n",
      "Accuracy = (47+97921)/(47+73+11778+97921) = 0.892086068895182"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "To better evaluate the performance of PCA and minmum description length based merging, we can make a better labeled dataset of higher resolution; redefine true negative and expected pairs of merged nodes, etc. "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<b>2. Kolmogorov\u2013Smirnov test based merging</b><br>\n",
      "Besides PCA and MDL based merging, we also explored a Kolmogorov\u2013Smirnov test based method to study anual trends of TMAX and TMIN in a global scale. We are interested in global warming, which is quite debatable. We selected station/year TMAX and TMIN record with at least 300 days and then aggreated each record, using a mean value to represent the maximum and minmum temperature of that year in a station. After this, we kept stations/nodes of more than 30 records and used K-S test to justify if the two datasets are from the same distribution. "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "K-S test code for TMIN"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%%writefile ksTest.py\n",
      "#!/usr/bin/python\n",
      "# Li's intermediate data\n",
      "# Input Format: station:year,[tmin,tmax]\n",
      "# Output Format: station:meas,[mean values]\n",
      "\n",
      "import sys\n",
      "sys.path.append('/usr/lib/python2.6/dist-packages')\n",
      "from mrjob.job import MRJob\n",
      "import mrjob\n",
      "from sys import stderr\n",
      "import re,pickle,base64,zlib\n",
      "import numpy as np\n",
      "from scipy import stats\n",
      "import pickle\n",
      "\n",
      "def NN(vec):\n",
      "    \"\"\"nearest neighbor method to substitute missing values in record the nearest left neighbor\"\"\"\n",
      "    # if there is None\n",
      "    if not None in vec: # None is the only representation of missing value\n",
      "        return vec\n",
      "    else:\n",
      "        for i in xrange(len(vec)):\n",
      "            if vec[i]==None:\n",
      "                if i>0: # if vec[i] is not the first item, make it equal to its left neighbor\n",
      "                    vec[i]=vec[i-1]\n",
      "                else:  # if vec[i] is the first item, make it equal to the first valid value in this list\n",
      "                    vec[i]=filter(None,vec)[0]\n",
      "        return vec\n",
      "\n",
      "def str2flt(vec):\n",
      "    \"\"\"convert string elements to float; missing values are replaced with None\"\"\"\n",
      "    newvec=[]\n",
      "    for v in vec:\n",
      "        try:\n",
      "            newv=float(v)\n",
      "        except:\n",
      "            newv=None\n",
      "        finally:\n",
      "            newvec.append(newv)\n",
      "    return newvec\n",
      "\n",
      "class ksTest(MRJob):\n",
      "    INPUT_PROTOCOL = mrjob.protocol.RawValueProtocol\n",
      "    INTERNAL_PROTOCOL = mrjob.protocol.PickleProtocol\n",
      "    OUTPUT_PROTOCOL = mrjob.protocol.JSONProtocol\n",
      "    \n",
      "    def datasplit_mapper(self,_,line):\n",
      "        try:\n",
      "            rec = line.split(\",\")\n",
      "            node=rec[0] # exisits a root node with node id ''\n",
      "            tminLen=int(rec[1])\n",
      "            tmaxLen=int(rec[2])\n",
      "            tmin = rec[3:3+tminLen]\n",
      "            tmin = [np.float(t) for t in tmin]\n",
      "            tmax = rec[3+tminLen:]\n",
      "            tmax = [np.float(t) for t in tmax]\n",
      "#             stderr.write(node+\"\\t\"+str(tmin[:3])+\"\\t\"+str(tmax[:3])+\"\\n\"\n",
      "            yield \"TMIN\",(node,tmin)\n",
      "#             yield (\"TMAX\",node),tmax\n",
      "        except Exception, e:\n",
      "            stderr.write(str(e))\n",
      "\n",
      "    def ksTest_reducer(self,key,value):\n",
      "        try:\n",
      "            meas=key\n",
      "            vals = list(value)\n",
      "            rec = {}\n",
      "            for v in vals:\n",
      "                if v[0]!='': # filter out the root node\n",
      "                    rec[v[0]]=v[1]\n",
      "            nodeList=rec.keys()\n",
      "#             stderr.write(\"len of nodeList:\\t\"+str(len(nodeList))+\"\\n\")\n",
      "            for key in nodeList:\n",
      "                sib = key[:-1]+str(1-int(key[-1]))\n",
      "                if sib in nodeList:\n",
      "#                     stderr.write(\"sid in nodeList:\\t\"+key+'\\t'+sib+'\\n')\n",
      "                    kst,p = stats.ks_2samp(rec[key],rec[sib])\n",
      "                    if p>0.01:\n",
      "                        yield (key,sib),p\n",
      "        except Exception, e:\n",
      "            stderr.write(str(e))\n",
      "\n",
      "    def steps(self):\n",
      "        return [\n",
      "            self.mr(mapper=self.datasplit_mapper,\n",
      "                    reducer=self.ksTest_reducer),\n",
      "        ]\n",
      "\n",
      "if __name__ == '__main__':\n",
      "    ksTest.run()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "!python ksTest.py -r emr --emr-job-flow-id  j-2S5LTMBIWD2WB s3://lge.bucket/weather/node-daily-avg-tminmax-all.csv > ksTest.out"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "ksTest.out format: [node1, node2],p-value of K-S test. Only p-value>0.01 are displayed. If p>0.01, we accept the null hypothesis that the two samples are from the same distribution."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "!cat ksTest.out"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[\"110001100\", \"110001101\"]\t0.042133297655674806\r\n",
        "[\"01100011\", \"01100010\"]\t0.079088349625565144\r\n",
        "[\"001110101\", \"001110100\"]\t0.042415495404584364\r\n",
        "[\"001110100\", \"001110101\"]\t0.042415495404584364\r\n",
        "[\"01110000\", \"01110001\"]\t0.021228364652015285\r\n",
        "[\"110010000\", \"110010001\"]\t0.038725337989893534\r\n",
        "[\"110010001\", \"110010000\"]\t0.038725337989893534\r\n",
        "[\"01100010\", \"01100011\"]\t0.079088349625565144\r\n",
        "[\"100000110\", \"100000111\"]\t0.13335886689487245\r\n",
        "[\"100000111\", \"100000110\"]\t0.13335886689487245\r\n",
        "[\"010111100\", \"010111101\"]\t0.04220964917906829\r\n",
        "[\"010111101\", \"010111100\"]\t0.04220964917906829\r\n",
        "[\"011110001\", \"011110000\"]\t0.010816931851251925\r\n",
        "[\"011101010\", \"011101011\"]\t0.020622966086130651\r\n",
        "[\"011101011\", \"011101010\"]\t0.020622966086130651\r\n",
        "[\"011110000\", \"011110001\"]\t0.010816931851251925\r\n",
        "[\"010011011\", \"010011010\"]\t0.033665342722160499\r\n",
        "[\"010011010\", \"010011011\"]\t0.033665342722160499\r\n",
        "[\"100010010\", \"100010011\"]\t0.38979169276214282\r\n",
        "[\"100010011\", \"100010010\"]\t0.38979169276214282\r\n",
        "[\"011010101\", \"011010100\"]\t0.25644295635781439\r\n",
        "[\"011010100\", \"011010101\"]\t0.25644295635781439\r\n",
        "[\"110001101\", \"110001100\"]\t0.042133297655674806\r\n",
        "[\"100001100\", \"100001101\"]\t0.087625370945111955\r\n",
        "[\"100001101\", \"100001100\"]\t0.087625370945111955\r\n",
        "[\"001111100\", \"001111101\"]\t0.055389148596239689\r\n",
        "[\"001111101\", \"001111100\"]\t0.055389148596239689\r\n",
        "[\"100111010\", \"100111011\"]\t0.033091934634645569\r\n",
        "[\"100111011\", \"100111010\"]\t0.033091934634645569\r\n",
        "[\"01110001\", \"01110000\"]\t0.021228364652015285\r\n",
        "[\"110100101\", \"110100100\"]\t0.011874973744384428\r\n",
        "[\"110100100\", \"110100101\"]\t0.011874973744384428\r\n"
       ]
      }
     ],
     "prompt_number": 28
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Altering threshold, we got different merges. If we set p=0.05 as the threshold, we got 20 merged nodes for TMAX and 6 merged nodes for TMIN. We didn't apply the same analysis as that for PCA outcome on K-S test, because of the small result set."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}