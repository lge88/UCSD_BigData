{
 "metadata": {
  "name": "",
  "signature": "sha256:fb181ba430fd8707840a004f63e466229e6226af9c8486b1a516ec726648f1c9"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<a name='top'></a>\n",
      "## Overview of the Workflow ##\n",
      "0. **Sample the Data for Testing**     \n",
      "    Sample the weather.csv using MapReduce.\n",
      "    - Input: \n",
      "    \n",
      "      s3://lge-bucket/weather-data/weather.csv            \n",
      "      \n",
      "    - Output: \n",
      "    \n",
      "      s3://lge-bucket/weather/weather-1-of-100.csv/\n",
      "      \n",
      "      s3://lge.bucket/weather/weather-1-of-1000.csv/\n",
      "                 \n",
      "    - Implementation:\n",
      "    \n",
      "      [mr_get_sample.py](#mr_get_sample)\n",
      "            \n",
      "      [run_mr_get_sample_weather_1_of_100.sh](#run_mr_get_sample_weather_1_of_10000)\n",
      "      \n",
      "      [run_mr_get_sample_weather_1_of_1000.sh](#run_mr_get_sample_weather_1_of_1000)\n",
      "      <a name='top-0'></a>\n",
      "      \n",
      "1. **Count the Number of Measurements**     \n",
      "    Count the number of valid records of TMIN and TMAX measurement for each station using MapReduce. In the implementation, if a record with more than half of the days in one year has valid data, we consider the record is valid.\n",
      "    - Input: \n",
      "    \n",
      "      s3://lge-bucket/weather-data/weather.csv            \n",
      "      \n",
      "    - Output: \n",
      "    \n",
      "      [station-measurement-counts.txt](#station-measurement-counts)\n",
      "          Format: \"station-id\" number_of_TMIN_or_TMAX_records\n",
      "          \n",
      "      s3://lge-bucket/weather-data/station-measurement-counts.txt (same as above)\n",
      "    - Implementation:\n",
      "    \n",
      "      [weather_data_parser.py](#weather_data_parser)\n",
      "      \n",
      "      [mr_station_measurement_count.py](#mr_station_measurement_count)\n",
      "      \n",
      "      [run_mr_station_measurement_count.sh](#run_mr_station_measurement_count)\n",
      "      <a name='top-2'></a>\n",
      "  \n",
      "2. **Join Stations and Weights** \n",
      "\n",
      "    Join the station data table with the number of valid records as a new column `weight'.\n",
      "    - Input: \n",
      "      \n",
      "      s3://Weather.GHNC/stations.pkl.gz\n",
      "              \n",
      "      [station-measurement-counts.txt](#station-measurement-counts)\n",
      "    - Output:\n",
      "    \n",
      "      [station-lat-lon-weight.csv](#station-lat-lon-weight)\n",
      "          Format: station-id, latitude, longitude, weight\n",
      "      \n",
      "      s3://lge-bucket/weather-data/station-lat-lon-weight.csv (same as above)\n",
      "    - Implementation:\n",
      "    \n",
      "      [join_station_with_weight.py](#join_station_with_weight)\n",
      "      <a name='top-3'></a>\n",
      "      \n",
      "3. **Spatial Partitioning**\n",
      "    <a name='spatial-partition'></a>\n",
      "    \n",
      "    Run k-d tree partitioning for the stations based on the weighted station data.\n",
      "    - Input: \n",
      "    \n",
      "      [stations-lat-lon-weight.csv](#station-lat-lon-weight)\n",
      "    - Output: \n",
      "    \n",
      "      [partition-tree.csv](#partition-tree)\n",
      "          Format: node-id, coordinate (either 'lat' or 'lon'), threshold (value of latitude or longitude)\n",
      "          \n",
      "      [station-to-node.csv](#station-to-node)\n",
      "          Format: station-id, node-id\n",
      "      \n",
      "    - Implementation: [geo_partition.py](#geo_partition)\n",
      "      <a name='top-4'></a>\n",
      "      \n",
      "4. **Concat TMIN and TMAX vector** \n",
      "\n",
      "    For the same station the same year, concatenate TMIN (1 x 365) and TMAX (1 x 365) vector to form a 1 x 730 vector. This is done using MapReduce.\n",
      "    - Input: \n",
      "      \n",
      "      s3://lge-bucket/weather-data/weather.csv\n",
      "    - Output: \n",
      "      s3://lge.bucket/weather/weather-tminmax/\n",
      "          Format: station-id:year, [data (1 x 730)]     \n",
      "    - Implementation:\n",
      "      \n",
      "      [mr_weather_concat_tminmax.py](#mr_weather_concat_tminmax)\n",
      "            \n",
      "      [run_mr_weather_concat_tminmax.sh](#run_mr_weather_concat_tminmax)\n",
      "      <a name='top-5'></a>\n",
      "5. **Linear Regression Analysis For Each station**\n",
      "\n",
      "    For each station, each year, compute the average temperature using the 730 x 1 vector. Then for each station, we will have a set of (year, average_temperature) data samples. Therefore, we can try to  use a linear mode to fit the average temperature (Y) vs year (X) data, i.e., Y = AX + b. If for most stations, A value (the slope) is larger than zero, we consider the global warming trend exists.\n",
      "    \n",
      "    - Input: \n",
      "    \n",
      "      s3://lge.bucket/weather/weather-tminmax/\n",
      "      \n",
      "    - Output:\n",
      "      \n",
      "      s3://lge.bucket/weather/weather-avg-temp-A-b-r-p-e/\n",
      "          Format: station-id, A, b, r-value, p-value, standard-error\n",
      "    \n",
      "    - Implementation:\n",
      "    \n",
      "      [mr_weather_avg_temp.py](#mr_weather_avg_temp)\n",
      "      <a name='top-avg-temp-regression'></a>\n",
      "\n",
      "6. **Compute Node Descriptors using PCA**\n",
      "\n",
      "    Compute the descriptor for each node at different level (0-9, 0 means the root level, 9 means leaf level) using MapReduce. Use 95% as the explained variance ration threshold.\n",
      "    - Input: \n",
      "      \n",
      "      s3://lge-bucket/weather-data/weather-tminmax/\n",
      "            \n",
      "      [station-to-node.csv](#station-to-node)\n",
      "    - Output: \n",
      "            \n",
      "      s3://lge-bucket/weather-data/node-descriptor-k-n-dl/ (withot vector data)\n",
      "          Format: node-id, k, num_of_samples, descriptor_length\n",
      "                \n",
      "      s3://lge-bucket/weather-data/node-descriptor/ (full)\n",
      "          Format: node-id, k, num_of_samples, descriptor_length, [mu], [D_k], [U_k]\n",
      "              mu is the mean vector, 1 x 730.\n",
      "              D_k is the first k singular values, 1 x k.\n",
      "              U_k is the first k eigen vectors, k x 1 x 730.\n",
      "    - Implementation:\n",
      "            \n",
      "      [geo_partition.py](#geo_partition): provides StationToNode data structure.\n",
      "      \n",
      "      [mr_weather_pca.py](#mr_weather_pca): [mr_weather_pca_usage](#mr_weather_pca_usage)\n",
      "      \n",
      "      [run_mr_weather_pca.sh](#run_mr_weather_pca)\n",
      "      <a name='top-6'></a>\n",
      "\n",
      "7. **Compute Daily Average Temperatures For Each Node** \n",
      "    \n",
      "    For each station's annual record, compute the average daily temperature. Then group the daily average temperature by node.\n",
      "    - Input:\n",
      "    \n",
      "      s3://lge.bucket/weather/weather-tminmax/\n",
      "      \n",
      "      [station-to-node.csv](#station-to-node)\n",
      "    - Output:\n",
      "    \n",
      "      s3://lge.bucket/weather/node-daily-avg-tminmax\n",
      "          Format: node-id, len_of_avg_tmins, len_of_avg_tmaxs, [tmins], [tmaxs]\n",
      "              tmins and tmaxs are vector of floats, their length are defined by len_of_avg_tmins, len_of_avg_tmaxs.\n",
      "\n",
      "8. **Merge Nodes** \n",
      "\n",
      "   ***By Level Based On Minimum Descriptor Length (MDL) Principle***\n",
      "    \n",
      "    For given level, check each pair of children under the same parent, if the sum of descriptor length from the children is larger than the descriptor length of the parent, then the children should merge.\n",
      "    - Input: \n",
      "      \n",
      "      s3://lge.bucket/weather/node-descriptor-k-n-dl/\n",
      "      \n",
      "      [partition-tree.csv](#partition-tree)\n",
      "    - Output:\n",
      "      \n",
      "      [node-pairs-should-merge-using-mdl.csv](#node-pairs-should-merge-using-mdl):\n",
      "          Format: left-node-id, right-node-id\n",
      "    - Implementation:\n",
      "    \n",
      "      [partition_node_merge.py](#partition_node_merge)\n",
      "      <a name='top-7'></a>\n",
      "      <a name='top-merge-by-level-mdl'></a>\n",
      "      \n",
      "   ***Using Undirected Graph and Minimum Descriptor Length (MDL) Principle***\n",
      "    \n",
      "    This algorithm adopts the idea that after we have the partition tree, we only keep the leaves as partitioned sections and throw away the concept tree structure. Then we generate a graph of the leaf nodes, basing on adjacency. The graph is going to be a undirected graph, and we could apply the node merging algorithm (which is going to be done locally) recursively on the graph until either every pair of mergable nodes has been merged, or we've exceeded a predefined limit of MAX_STEP. Details about how to determine the adjacency, and how to represent a merged node without going into a tedious job of describing the boundary of the polygon we have after merging, are explained in the notebook below.\n",
      "    \n",
      "    - Input: \n",
      "          The partition_tree we have. Currently we have two version of partition. For the purpose of demo, we used the professor's partition file. Our partition algorithm can be found in an ealier section of the report as well.\n",
      "      \n",
      "    - Output:\n",
      "    \n",
      "      s3://yuf/data/old_group.txt\n",
      "          Format: station_id  node_id\n",
      "              station_id is the name of each station we get from the input file,\n",
      "              node_id is the arbitrary id we assign to each leave node after the partitiion.\n",
      "\n",
      "      s3://yuf/data/new_group.txt\n",
      "          Format: station_id  node_id, with same meaning as above.\n",
      "\n",
      "    - Implementation:\n",
      "    \n",
      "      http://nbviewer.ipython.org/github/74s0nf/291_final/blob/master/final.ipynb\n",
      "      \n",
      "   \n",
      "   ***By Level Based On Daily Average Temperature Measturements***\n",
      "    \n",
      "    For given level, check each pair of children under the same parent, excute the kstest on daily average tmin and tmax sample data of the pair of children, if the p value returned by kstest is greater than 0.05, the average tmin (or tmax) distributions of the twon children are considered to be similar and therefore should be merged.\n",
      "    \n",
      "    - Input: \n",
      "    \n",
      "      s3://lge.bucket/weather/node-daily-avg-tminmax-all.csv\n",
      "    - Output:\n",
      "\n",
      "   [node-pairs-should-merge-using-kstest-tmax.csv](#node-pairs-should-merge-using-kstest)\n",
      "   \n",
      "   [node-pairs-should-merge-using-kstest-tmin.csv](#node-pairs-should-merge-using-kstest)\n",
      "    - Implementation:\n",
      "       \n",
      "      [ks_test_merge.py](#ks_test_merge)\n",
      "      <a name='top-merge-by-level-kstest'></a>"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Results & Analysis ##\n",
      "\n",
      "### Web GUI ###\n",
      "Results are visualized in google map: [http://lge88.github.io/weather-map/](http://lge88.github.io/weather-map/). With the GUI, user can:\n",
      "\n",
      "- Toggle stations visibility.\n",
      "- Change the station marker style (weather using circle or not).\n",
      "- Toggle the visibility of the stations density map.\n",
      "- Toggle node merge (show the result before/after merging).\n",
      "- Choose the level to inspect. Level is a integer from 0 to 9. 0 means no partition. 9 is the finest partition.\n",
      "- Choose sample ratio. The portion of the whole stations file to view. Showing all 85284 stations on google map makes it realy slow.\n",
      "- Shuffle colors. Shuffle the color of markers randomly.\n",
      "- Visualize the result of linear regression result of daily average temperature over year. Choose different value type to show."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Evaluation ###"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "\n",
      "In order to determine how good our model is when used to describe the global weather pattern, we create a 'golden standard' that we believe can well explalin the gobal weather patterns. And we tested the two mathematical models: PCA and Kolmogorov\u2013Smirnov test. Details of the evaluation could be found in the following notebook:\n",
      "\n",
      "http://nbviewer.ipython.org/github/lge88/UCSD_BigData/blob/master/notebooks/lge-hw2/Evaluation.ipynb\n",
      "    "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Conclusion & Future Work ###\n",
      "\n",
      "In the future, one might be interested in applying the work flow we've created to new sets of data. Also, to find the most relavent descriptors of weather pattern, one might want to apply the job flow to records besides the weather, or precipitation. Some combined effects of multiple reasons might also be explored."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Difficultes in the Implementation ##\n",
      "\n",
      "The code works on both local and emr when processing the sampled datasets. But when it applies to the full size data, the tasks failed. Therefore the visualization of the results is based on the result of sampled dataset (1-of-100). \n",
      "\n",
      "By inspecting the error logs, there are two major reason causes the job to fail. \n",
      "\n",
      "The first one was the memory error. It occurs in the reducer of the first step. The cause was try to load all vector data into the memory when compute mean vector and then recenter each vector according to its mean vector.\n",
      "    # Pseudocode\n",
      "    vecs = list(vecs)\n",
      "    # vecs was a generator, but now it is a list fully loaded into the memory\n",
      "    # this might huge if the number of vecs are large\n",
      "    mu = compute_vec_mean(vecs)\n",
      "    for vec in vecs:\n",
      "        vec -= mu\n",
      "        yield node, vec\n",
      "       \n",
      "The issue was fixed by writing the vectors line by line into a temp file. After the mean vector is computed, read the vectors from file and process line by line.\n",
      "    # Pseudocode\n",
      "    mu = zero_vector()\n",
      "    n = 0\n",
      "    for vec in vecs:\n",
      "        write_vector_to(tmp_file, encode_vec(vec))\n",
      "        n += 1\n",
      "        mu += vec\n",
      "    mu /= n\n",
      "    \n",
      "    for line in read_vector_from(tmp_file):\n",
      "        vec = decode(line)\n",
      "        vec -= mu\n",
      "        yield node, vec\n",
      "    \n",
      "\n",
      "The second the issue might be the disk space limitation. This cause is not yet full identified. By the task error log, the following exceptions are found:\n",
      "\n",
      "    java.lang.RuntimeException: java.io.IOException: Spill failed\n",
      "\n",
      "By searching the web, [stack overflow](http://stackoverflow.com/questions/11602074/hadoop-job-runs-okay-on-smaller-set-of-data-but-fails-with-large-dataset) indicates the disk space limitation might be the cause. If that is the case, two ways might be tried in the future:\n",
      "\n",
      "- Compress before yielding the values\n",
      "- Choose machines with larger disk space"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Intermediate Results ##"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#### station-measurement-counts.txt\n",
      "[back to top](#top) <a name='station-measurement-counts'></a>"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "!head -10 data/station-measurement-counts.txt"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\"AJ000037756\"\t4\r\n",
        "\"AJ000037907\"\t42\r\n",
        "\"AQC00914873\"\t18\r\n",
        "\"AR000087270\"\t95\r\n",
        "\"AR000087344\"\t103\r\n",
        "\"AR000087418\"\t100\r\n",
        "\"AR000087715\"\t106\r\n",
        "\"AR000087803\"\t97\r\n",
        "\"AR000870470\"\t94\r\n",
        "\"ASN00001019\"\t28\r\n"
       ]
      }
     ],
     "prompt_number": 7
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#### station-lat-lon-weight.csv\n",
      "[back to top](#top-2) <a name='station-lat-lon-weight'></a>"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "!head -10 data/station-lat-lon-weight.csv"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "ACW00011604,17.1167,-61.7833,2\r\n",
        "ACW00011647,17.1333,-61.7833,0\r\n",
        "AE000041196,25.333,55.517,83\r\n",
        "AF000040930,35.317,69.017,3\r\n",
        "AG000060390,36.7167,3.25,146\r\n",
        "AG000060590,30.5667,2.8667,146\r\n",
        "AG000060611,28.05,9.6331,108\r\n",
        "AG000060680,22.8,5.4331,146\r\n",
        "AGE00135039,35.7297,0.65,126\r\n",
        "AJ000037575,41.55,46.667,71\r\n"
       ]
      }
     ],
     "prompt_number": 6
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#### partition-tree.csv\n",
      "[back to top](#top-3) <a name='partition-tree'></a>"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "!head -10 data/partition-tree.csv\n",
      "!echo ...\n",
      "!tail -10 data/partition-tree.csv"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        ",lon,-90.7964\r\n",
        "0,lat,41.4917\r\n",
        "00,lon,-104.6822\r\n",
        "000,lat,37.0856\r\n",
        "0000,lon,-112.8003\r\n",
        "00000,lat,34.0697\r\n",
        "000000,lon,-117.9789\r\n",
        "0000000,lat,21.1539\r\n",
        "00000000,lon,-155.9117\r\n",
        "000000000,lat,20.7583\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "...\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "11111101,lon,66.05\r\n",
        "111111010,lat,65.967\r\n",
        "111111011,lat,67.433\r\n",
        "1111111,lat,62.833\r\n",
        "11111110,lon,129.717\r\n",
        "111111100,lat,59.283\r\n",
        "111111101,lat,59.917\r\n",
        "11111111,lon,136.217\r\n",
        "111111110,lat,67.55\r\n",
        "111111111,lat,65.733\r\n"
       ]
      }
     ],
     "prompt_number": 10
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#### node-pairs-should-merge-using-mdl.csv\n",
      "[back to top](#top-merge-by-level-mdl) <a name='node-pairs-should-merge-using-mdl'></a>"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "!head -10 data/node-pairs-should-merge-using-mdl.csv"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#### node-pairs-should-merge-kstest-using-tmin.csv\n",
      "[back to top](#top-merge-by-level-kstest) <a name='node-pairs-should-merge-using-kstest'></a>"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#### node-pairs-should-merge-kstest-using-tmin.csv\n",
      "[back to top](#top-merge-by-level-kstest) <a name='node-pairs-should-merge-kstest-using-tmin'></a>"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "!head -10 data/node-pairs-should-merge-using-kstest-tmin.csv"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "01100010,01100011\r\n",
        "100000110,100000111\r\n",
        "011010100,011010101\r\n",
        "100010010,100010011\r\n",
        "100001100,100001101\r\n",
        "001111100,001111101\r\n"
       ]
      }
     ],
     "prompt_number": 6
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#### node-pairs-should-merge-kstest-using-tmax.csv\n",
      "[back to top](#top-merge-by-level-kstest) <a name='node-pairs-should-merge-kstest-using-tmax'></a>"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "!head -10 data/node-pairs-should-merge-using-kstest-tmax.csv"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "001100100,001100101\r\n",
        "000110000,000110001\r\n",
        "011011010,011011011\r\n",
        "011011110,011011111\r\n",
        "001101100,001101101\r\n",
        "100100000,100100001\r\n",
        "100101000,100101001\r\n",
        "101001100,101001101\r\n",
        "110001010,110001011\r\n",
        "001001110,001001111\r\n"
       ]
      }
     ],
     "prompt_number": 5
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#### station-to-node.csv\n",
      "[back to top](#top-3) <a name='station-to-node'></a>"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "!head -10 data/station-to-node.csv"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "USC00515000,000000000\r\n",
        "KRC00914101,000000000\r\n",
        "USC00514725,000000000\r\n",
        "AQC00914822,000000000\r\n",
        "USC00511008,000000000\r\n",
        "USC00517000,000000000\r\n",
        "JQW00021602,000000000\r\n",
        "USC00515006,000000000\r\n",
        "USC00512558,000000000\r\n",
        "AQC00914873,000000000\r\n"
       ]
      }
     ],
     "prompt_number": 11
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#### partition-tree-nid-coord-thres-k-n-dl-gid.csv\n",
      "[back to top](#top-6) <a name='partition-tree-nid-coord-thres-k-n-dl-gid'></a>"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "!head -10 data/partition-tree-nid-coord-thres-k-n-dl-gid-1-of-100.csv"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        ",lon,-90.7964,532,39230,21259450,0\r\n",
        "10110001,lon,-77.8167,103,124,88692,177\r\n",
        "110000,lon,-84.7667,367,668,513796,49\r\n",
        "110001,lon,-84.6333,371,644,510484,49\r\n",
        "10110000,lon,-77.9667,112,138,97946,177\r\n",
        "1101001,lat,43.98,187,244,182868,106\r\n",
        "010,lat,48.0667,530,4518,2782170,3\r\n",
        "01000000,lon,-123.3572,110,153,97860,65\r\n",
        "101011,lon,149.5242,247,420,284780,43\r\n",
        "110110100,lat,43.9522,72,85,59410,437\r\n"
       ]
      }
     ],
     "prompt_number": 2
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Implementation ##"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#### mr_get_sample.py\n",
      "[back to top](#top) <a name='mr_get_sample'></a>"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%load mr_get_sample.py"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 12
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "\n",
      "from sys import stderr\n",
      "from mrjob.job import MRJob\n",
      "from mrjob.protocol import RawValueProtocol\n",
      "from random import random\n",
      "\n",
      "class MRGetSample(MRJob):\n",
      "  OUTPUT_PROTOCOL = RawValueProtocol\n",
      "\n",
      "  def configure_options(self):\n",
      "    super(MRGetSample, self).configure_options()\n",
      "    self.add_passthrough_option(\n",
      "      '--probability',\n",
      "      type='float',\n",
      "      default=0.01,\n",
      "      help='Sample probability')\n",
      "\n",
      "  def mapper(self, _, line):\n",
      "    if random() < self.options.probability:\n",
      "      yield None, line\n",
      "\n",
      "  def reducer(self, _, lines):\n",
      "    for line in lines:\n",
      "      yield None, line\n",
      "\n",
      "if __name__ == '__main__':\n",
      "  MRGetSample.run()\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      " run_mr_weather_get_sample_1_of_100.sh"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 13
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#! /bin/sh\n",
      "# JOB_FLOW_ID='j-34Q4HR41T9BWU'\n",
      "\n",
      "# python mr_get_sample.py -r emr --emr-job-flow-id ${JOB_FLOW_ID} \\\n",
      "#   -c ~/.mrjob.iam.conf \\\n",
      "#   --probability=0.01 \\\n",
      "#   --output-dir=s3://lge.bucket/weather/weather-1-of-100.csv/ \\\n",
      "#   --no-output \\\n",
      "#   hdfs:/weather.raw_data/ALL.csv\n",
      "\n",
      "#! /bin/sh\n",
      "JOB_FLOW_ID='j-31PKW9W4C2YQK'\n",
      "\n",
      "python mr_get_sample.py -r emr --emr-job-flow-id ${JOB_FLOW_ID} \\\n",
      "  -c ~/.mrjob.iam.conf \\\n",
      "  --probability=0.01 \\\n",
      "  --output-dir=s3://lge.bucket/weather/weather-tminmax-1-of-100/ \\\n",
      "  --no-output \\\n",
      "  s3://lge.bucket/weather/weather-tminmax/\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#### weather_data_parser.py\n",
      "[back to top](#top) <a name='weather_data_parser'></a>"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%load weather_data_parser.py"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 13
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "\n",
      "class WeatherDataParser:\n",
      "  DAYS = 365\n",
      "\n",
      "  def __init__(self,\n",
      "               default_int_val = None,\n",
      "               good_data_ratio_threshold = 0.5,\n",
      "               enable_post_processing = True,\n",
      "               selected_days = None):\n",
      "    self.default_int_val = default_int_val\n",
      "    self.good_data_ratio_threshold = good_data_ratio_threshold\n",
      "    self.enable_post_processing = enable_post_processing\n",
      "\n",
      "    if selected_days is None:\n",
      "      self.selected_days = range(WeatherDataParser.DAYS)\n",
      "    else:\n",
      "      self.selected_days = set([day for day in selected_days if day >= 0 and day < WeatherDataParser.DAYS])\n",
      "\n",
      "  def parse_int(self, x):\n",
      "    try:\n",
      "      return int(x)\n",
      "    except ValueError:\n",
      "      return self.default_int_val\n",
      "\n",
      "  def post_process_data(self, data):\n",
      "    def find_nearest_neighbor(data, indx):\n",
      "      offset, days = 1, WeatherDataParser.DAYS\n",
      "      half_days = days / 2\n",
      "      while offset <= half_days:\n",
      "        r_indx = (indx + offset) % days\n",
      "        val = data[r_indx]\n",
      "        if val is not None: return val\n",
      "\n",
      "        l_indx = (indx - offset) % days\n",
      "        val = data[l_indx]\n",
      "        if val is not None: return val\n",
      "\n",
      "        offset += 1\n",
      "      return self.default_int_val\n",
      "      # raise Exception('Should never reach hear with proper good data ratio threshold!')\n",
      "\n",
      "    for i, x in enumerate(data):\n",
      "      if x is None:\n",
      "        data[i] = find_nearest_neighbor(data, i)\n",
      "\n",
      "  def parse_line(self, line):\n",
      "    vec = line.strip().split(',')\n",
      "    station = vec[0]\n",
      "    measurement = vec[1]\n",
      "    year = self.parse_int(vec[2])\n",
      "\n",
      "    data, num_of_records = [], 0\n",
      "\n",
      "    for day in self.selected_days:\n",
      "      item = vec[3 + day]\n",
      "      x = self.parse_int(item)\n",
      "      if x is not None: num_of_records += 1\n",
      "      data.append(x)\n",
      "\n",
      "    num_of_days = len(self.selected_days)\n",
      "\n",
      "    if num_of_days == 0 or float(num_of_records) / num_of_days < self.good_data_ratio_threshold:\n",
      "      return None\n",
      "    else:\n",
      "      if self.enable_post_processing: self.post_process_data(data)\n",
      "      return (station, year, measurement, data, num_of_records)\n",
      "\n",
      "if __name__ == '__main__':\n",
      "  wdp = WeatherDataParser(selected_days=range(20))\n",
      "\n",
      "  with open('./weather-923-of-9358395-shuffled.csv') as f:\n",
      "    print 1, wdp.parse_line(f.readline())\n",
      "    print 2, wdp.parse_line(f.readline())\n",
      "    print 3, wdp.parse_line(f.readline())\n",
      "    print 4, wdp.parse_line(f.readline())\n",
      "    print 5, wdp.parse_line(f.readline())\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#### mr_station_measurement_count.py\n",
      "[back to top](#top) <a name='mr_station_measurement_count'></a>"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%load mr_station_measurement_count.py"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 14
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "\n",
      "from mrjob.job import MRJob\n",
      "from weather_data_parser import WeatherDataParser\n",
      "\n",
      "class MRStationMeasurementCount(MRJob):\n",
      "  def mapper_init(self):\n",
      "    self.parser = WeatherDataParser()\n",
      "\n",
      "  def mapper(self, _, line):\n",
      "    res = self.parser.parse_line(line)\n",
      "    if res is not None:\n",
      "      station, year, measurement, data, num_of_records = res\n",
      "      if measurement == 'TMIN' or measurement == 'TMAX':\n",
      "        yield station, 1\n",
      "\n",
      "  def combiner(self, key, vals):\n",
      "    yield key, sum(vals)\n",
      "\n",
      "  def reducer(self, key, vals):\n",
      "    yield key, sum(vals)\n",
      "\n",
      "if __name__ == '__main__':\n",
      "  MRStationMeasurementCount.run()\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#### run_mr_station_measurement_count.sh\n",
      "[back to top](#top) <a name='run_mr_station_measurement_count'></a>"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%load run_mr_station_measurement_count.sh"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 16
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#! /bin/sh\n",
      "JOB_FLOW_ID='j-XF54NKBXHHP0'\n",
      "\n",
      "MODE=$1\n",
      "if [[ $MODE == '' ]]; then\n",
      "  MODE=local\n",
      "fi\n",
      "\n",
      "if [[ $MODE == 'emr' ]]; then\n",
      "  python mr_station_measurement_count.py -r emr \\\n",
      "    --emr-job-flow-id ${JOB_FLOW_ID} \\\n",
      "    -c ~/.mrjob.conf \\\n",
      "    s3://lge-bucket/weather-data/weather.csv > data/station-measurement-counts.txt\n",
      "else\n",
      "  python mr_station_measurement_count.py \\\n",
      "    data/weather-923-of-9358395-shuffled.csv\n",
      "fi"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#### join_station_with_weight.py\n",
      "[back to top](#top-2) <a name='join_station_with_weight'></a>"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%load join_station_with_weight.py"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 4
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "\n",
      "import pickle, re\n",
      "\n",
      "def join_stations_measurement_counts(filename, stations):\n",
      "  LINE_RE = '^\"(.*)\"\\t(.*)$'\n",
      "  stations['weight'] = 0.0\n",
      "  with open(filename, 'r') as f:\n",
      "    for line in f.readlines():\n",
      "      m = re.match(LINE_RE, line.strip())\n",
      "      if m is not None:\n",
      "        station, counts = m.group(1), int(m.group(2))\n",
      "        if station in stations.index:\n",
      "          stations.loc[station, 'weight'] = counts\n",
      "  return df_to_list(stations[['latitude', 'longitude', 'weight']])\n",
      "\n",
      "def df_to_dict(df):\n",
      "  out = {}\n",
      "  for indx in df.index:\n",
      "    item = df.ix[indx]\n",
      "    out[indx] = (item.latitude, item.longitude, item.weight)\n",
      "  return out\n",
      "\n",
      "def df_to_list(df):\n",
      "  out = []\n",
      "  for indx in df.index:\n",
      "    item = df.ix[indx]\n",
      "    out.append((str(indx), float(item.latitude), float(item.longitude), int(item.weight)))\n",
      "  return out\n",
      "\n",
      "def export_pickle_data(obj, filename):\n",
      "  with open(filename, 'wb') as f:\n",
      "    pickle.dump(obj, f)\n",
      "\n",
      "def export_text_data(lst, filename):\n",
      "  with open(filename, 'wb') as f:\n",
      "    for item in lst:\n",
      "      f.write(','.join(map(str, item)));\n",
      "      f.write('\\n');\n",
      "\n",
      "def load_pickle_data(filename):\n",
      "  with open(filename, 'rb') as f:\n",
      "    return pickle.load(f)\n",
      "\n",
      "if __name__ == '__main__':\n",
      "  s1 = load_pickle_data('stations.pkl')\n",
      "  s2 = join_stations_measurement_counts('data/station-measurement-counts.txt', s1)\n",
      "  export_text_data(s2, 'data/station-lat-lon-weight.csv')\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#### geo_partition.py\n",
      "[back to top](#top-3) <a name='geo_partition'></a>"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%load geo_partition.py"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 12
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import pickle, re, random\n",
      "\n",
      "class Station:\n",
      "  def __init__(self, name, lat, lon, weight):\n",
      "    self.name = str(name)\n",
      "    self.lat = float(lat)\n",
      "    self.lon = float(lon)\n",
      "    self.weight = int(weight)\n",
      "\n",
      "  def __repr__(self):\n",
      "    return '\\n'.join([\n",
      "      'name: ' + str(self.name),\n",
      "      'lat: ' + str(self.lat),\n",
      "      'lon: ' + str(self.lon),\n",
      "      'weight: ' + str(self.weight),\n",
      "    ])\n",
      "\n",
      "class StationToNodeTable:\n",
      "  def __init__(self):\n",
      "    self._table = {}\n",
      "\n",
      "  def read_from_csv_file(self, fname):\n",
      "    with open(fname, 'r') as f:\n",
      "      for line in f.readlines():\n",
      "        items = line.strip().split(',')\n",
      "        if len(items) == 2:\n",
      "          station, node = items\n",
      "          if station and node:\n",
      "            self._table[station] = node\n",
      "\n",
      "  def size(self):\n",
      "    return len(self._table)\n",
      "\n",
      "  def find_node(self, station_name):\n",
      "    return self._table[station_name] if self._table.has_key(station_name) else None\n",
      "\n",
      "  def print_sample(self, N=10):\n",
      "    ks = random.sample(self._table, N)\n",
      "    for k in ks: print k, self._table[k]\n",
      "\n",
      "def sort_stations_by_lat(stations):\n",
      "  stations.sort(key=lambda x: x.lat)\n",
      "\n",
      "def sort_stations_by_lon(stations):\n",
      "  stations.sort(key=lambda x: x.lon)\n",
      "\n",
      "def find_weighted_median_index(stations):\n",
      "  total_weight = reduce(lambda sofar, item: sofar + item.weight, stations, 0)\n",
      "  half_weight = total_weight / 2\n",
      "\n",
      "  # binary search in acc array will be faster than linear search\n",
      "  acc = 0\n",
      "  for i in range(len(stations)):\n",
      "    acc += stations[i].weight\n",
      "    if (acc >= half_weight): break\n",
      "\n",
      "  return i\n",
      "\n",
      "\n",
      "# TODO: implement partition algo based on station weights\n",
      "# For now, use yoav's partitioned data structure.\n",
      "def partition(stations, direction='lat'):\n",
      "  if direction != 'lon': direction = 'lat'\n",
      "  if direction == 'lat': sort_stations_by_lat(stations)\n",
      "  else: sort_stations_by_lon(stations)\n",
      "  indx = find_weighted_median_index(stations)\n",
      "  return (indx, stations[i], stations[:i], stations[i+1:])\n",
      "\n",
      "class TreeNode:\n",
      "  def __init__(self):\n",
      "    self.parent = None\n",
      "    self.stations = []\n",
      "    self.children = []\n",
      "\n",
      "  def is_leaf(self):\n",
      "    return len(self.children) == 0\n",
      "\n",
      "def write_pickle(obj, fname):\n",
      "  with open(fname, 'wb') as f: pickle.dump(obj, f)\n",
      "\n",
      "def read_pickle(fname):\n",
      "  with open(fname, 'rb') as f: return pickle.load(f)\n",
      "\n",
      "def read_csv(fname):\n",
      "  with open(fname) as f:\n",
      "    return [Station(*line.strip().split(',')) for line in f.readlines()]\n",
      "\n",
      "def partition(stations):\n",
      "  root = TreeNode()\n",
      "  # build kd-tree from stations\n",
      "  return root\n",
      "\n",
      "\n",
      "\n",
      "if __name__ == '__main__':\n",
      "  pass\n",
      "  # stations = read_csv('station-lat-lon-weight.csv')\n",
      "  # sort_stations_by_lat(stations)\n",
      "  # print find_weighted_median(stations)\n",
      "\n",
      "  # station_to_node = StationToNodeTable()\n",
      "  # station_to_node.read_from_csv_file('station-to-node-table-yoav.csv')\n",
      "  # station_to_node.print_sample()\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#### mr_weather_concat_tminmax.py\n",
      "[back to top](#top-4) <a name='mr_weather_concat_tminmax'></a>"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%load mr_weather_concat_tminmax.py"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 12
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "\n",
      "from mrjob.job import MRJob\n",
      "from sys import stderr\n",
      "from weather_data_parser import WeatherDataParser\n",
      "\n",
      "class MRConcatTminTmax(MRJob):\n",
      "\n",
      "  parser = None\n",
      "\n",
      "  def parse_line(self, line):\n",
      "    return MRConcatTminTmax.parser.parse_line(line)\n",
      "\n",
      "  def concat_tmin_tmax_mapper(self, _, line):\n",
      "    res = self.parse_line(line)\n",
      "\n",
      "    # stderr.write('line: ' + line + '\\n')\n",
      "    if res is not None:\n",
      "      station, year, measurement, vec, nr = res\n",
      "      if measurement == 'TMAX' or measurement == 'TMIN':\n",
      "        key = station + ':' + str(year)\n",
      "        # stderr.write('key: ' + key + '\\n')\n",
      "        stderr.write('key: ' + key + ' n: ' + str(nr) + '\\n')\n",
      "        # self.increment_counter('mapper', station)\n",
      "        yield key, (measurement, vec)\n",
      "\n",
      "  def concat_tmin_tmax_reducer(self, key, vals):\n",
      "    out = None\n",
      "    vals = list(vals)\n",
      "    # stderr.write('key: ' + key + ' vals: ' + str(vals) + '\\n')\n",
      "    for v in vals:\n",
      "      if out is None:\n",
      "        out = v[1]\n",
      "      else:\n",
      "        if v[0] == 'TMIN':\n",
      "          out = v[1] + out\n",
      "        else:\n",
      "          out = out + v[1]\n",
      "        stderr.write(key + ' is done.\\n')\n",
      "        yield key, out\n",
      "\n",
      "  def steps(self):\n",
      "    return [\n",
      "      self.mr(mapper=self.concat_tmin_tmax_mapper,\n",
      "              reducer=self.concat_tmin_tmax_reducer)\n",
      "    ]\n",
      "\n",
      "if __name__ == '__main__':\n",
      "  MRConcatTminTmax.parser = WeatherDataParser()\n",
      "  MRConcatTminTmax.run()\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#### run_mr_weather_concat_tminmax.sh\n",
      "[back to top](#top-4) <a name='run_mr_weather_concat_tminmax'></a>"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%load run_mr_weather_concat_tminmax.sh"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 13
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#! /bin/sh\n",
      "python mr_concat_tmin_tmax.py -r emr --emr-job-flow-id j-32917857IT6TB \\\n",
      "  --setup 'export PYTHONPATH=$PYTHONPATH:weather_data_parser.tar.gz#/' \\\n",
      "  --output-dir s3://weather-analysis/weather-tminmax --no-output \\\n",
      "  s3://lge-bucket/weather-data/weather.csv\n",
      "\n",
      "# python mr_concat_tmin_tmax.py weather-tmin-tmax-tweak-10.csv\n",
      "# python mr_concat_tmin_tmax.py s3://lge-bucket/weather-data/weather-9424-of-9358395.csv\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#### mr_weather_pca.py\n",
      "[back to top](#top-5) <a name='mr_weather_pca'></a>"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%load mr_weather_pca.py"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 14
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sys import stderr\n",
      "from os import remove\n",
      "from os.path import basename\n",
      "import pickle\n",
      "from mrjob.job import MRJob\n",
      "from mrjob.protocol import JSONValueProtocol, RawValueProtocol, JSONProtocol, PickleProtocol\n",
      "import numpy as np\n",
      "from numpy.linalg import svd\n",
      "\n",
      "from geo_partition import StationToNodeTable\n",
      "\n",
      "def parse_natural_int(i):\n",
      "  try:\n",
      "    i = int(i)\n",
      "  except ValueError:\n",
      "    return 0\n",
      "  return i if i >= 0 else 0\n",
      "\n",
      "def get_node_ancestor_at_level(node, level):\n",
      "  level = parse_natural_int(level)\n",
      "  return node[:level]\n",
      "\n",
      "class MRPCA(MRJob):\n",
      "  INPUT_PROTOCOL = JSONProtocol\n",
      "  INTERNAL_PROTOCOL = PickleProtocol\n",
      "  OUTPUT_PROTOCOL = JSONValueProtocol\n",
      "\n",
      "  TYPES = {\n",
      "    'mu': 0,\n",
      "    'centered_vector': 1\n",
      "  }\n",
      "\n",
      "  def debug(self, obj):\n",
      "    if not self.options.no_debug: stderr.write(str(obj) + '\\n');\n",
      "\n",
      "  def _debug_options(self):\n",
      "    self.debug('--station-to-node: ')\n",
      "    self.debug(self.options.station_to_node)\n",
      "\n",
      "    self.debug('--no-debug: ')\n",
      "    self.debug(self.options.no_debug)\n",
      "\n",
      "    self.debug('--reduced-dimension: ')\n",
      "    self.debug(self.options.reduced_dimension)\n",
      "\n",
      "    self.debug('--explained-variance-ratio-threshold: ')\n",
      "    self.debug(self.options.explained_variance_ratio_threshold)\n",
      "\n",
      "    self.debug('--num-levels: ')\n",
      "    self.debug(self.options.num_levels)\n",
      "\n",
      "    self.debug('--store-mu: ')\n",
      "    self.debug(self.options.store_mu)\n",
      "\n",
      "    self.debug('--store-eigen-vectors: ')\n",
      "    self.debug(self.options.store_eigen_vectors)\n",
      "\n",
      "    self.debug('--station-to-node: ')\n",
      "    self.debug(self.options.station_to_node)\n",
      "\n",
      "  def node_mapper_init(self):\n",
      "    self._debug_options()\n",
      "    self.station_to_node_table = StationToNodeTable()\n",
      "    self.station_to_node_table.read_from_csv_file(basename(self.options.station_to_node))\n",
      "\n",
      "  def node_mapper(self, key, vec):\n",
      "    # self.debug('in mapper 1, key ' + key)\n",
      "\n",
      "    station, year = key.split(':')\n",
      "    leaf = self.station_to_node_table.find_node(station)\n",
      "\n",
      "    vec = np.array(vec)\n",
      "\n",
      "    # Yield vec to the corresponding leaf node and all of its\n",
      "    # ancestors.\n",
      "    level = self.options.num_levels\n",
      "    # level = MRPCA.NUM_OF_LEVELS\n",
      "    while level >= 0:\n",
      "      node = get_node_ancestor_at_level(leaf, level)\n",
      "      level -= 1\n",
      "      if self.options.reduced_dimension:\n",
      "        yield node, vec[:self.options.reduced_dimension]\n",
      "      else:\n",
      "        yield node, vec\n",
      "\n",
      "  def node_mean_reducer(self, node, vecs):\n",
      "    # self.debug('in mean reducer, node ' + node)\n",
      "\n",
      "    mu, n = None, 0\n",
      "    for vec in vecs:\n",
      "      if mu is None:\n",
      "        mu = vec.copy()\n",
      "      else:\n",
      "        mu += vec\n",
      "      n += 1\n",
      "    mu /= n\n",
      "\n",
      "  def node_substract_mean_reducer(self, node, vecs):\n",
      "    # self.debug('in reducer 1, node ' + node)\n",
      "\n",
      "    # I hope vecs can be hold in memory, but no :(\n",
      "    # It will complain memory problem.\n",
      "    # vecs = list(vecs)\n",
      "    # mu = np.mean(vecs)\n",
      "\n",
      "    # Solution is to write to local file first:\n",
      "    # Once mu is computed, read them back.\n",
      "    mu, n = None, 0\n",
      "    fname = 'vecs-' + node\n",
      "    with open(fname, 'w') as f:\n",
      "      for vec in vecs:\n",
      "        if mu is None:\n",
      "          mu = vec.copy()\n",
      "        else:\n",
      "          mu += vec\n",
      "        line = ','.join(map(str, vec.tolist()))\n",
      "        f.write(line + '\\n')\n",
      "        n += 1\n",
      "\n",
      "    mu /= float(n)\n",
      "\n",
      "    # Now we have mu, process the vecs:\n",
      "    with open(fname, 'r') as f:\n",
      "      for line in f:\n",
      "        vec = np.array(map(float, line.strip().split(',')))\n",
      "        vec -= mu\n",
      "        yield node, (MRPCA.TYPES['centered_vector'], vec)\n",
      "\n",
      "    if self.options.store_mu:\n",
      "      yield node, (MRPCA.TYPES['mu'], mu)\n",
      "\n",
      "    remove(fname)\n",
      "\n",
      "  def node_descriptor_reducer(self, node, vals):\n",
      "    # self.debug('in reducer 2, node ' + node)\n",
      "\n",
      "    cov = None\n",
      "    mu = None\n",
      "    nsamples = 0\n",
      "\n",
      "    for val in vals:\n",
      "      if val[0] == MRPCA.TYPES['mu']:\n",
      "        mu = val[1]\n",
      "      elif val[0] == MRPCA.TYPES['centered_vector']:\n",
      "        vec = val[1]\n",
      "        m = np.outer(vec, vec)\n",
      "        nsamples += 1\n",
      "        if cov is None:\n",
      "          cov = m\n",
      "        else:\n",
      "          cov += m\n",
      "    cov /= nsamples\n",
      "\n",
      "    ratio = self.options.explained_variance_ratio_threshold\n",
      "    U, D, V = svd(cov)\n",
      "    k, sofar, total = 1, 0.0, sum(D)\n",
      "    sofar += D[k-1]\n",
      "    while sofar < ratio * total:\n",
      "      k += 1\n",
      "      sofar += D[k-1]\n",
      "\n",
      "    desc_len = nsamples * k + (k + 1) * 730\n",
      "\n",
      "    # Dimension reduced descriptor for node.\n",
      "    # Only care about decriptor length\n",
      "    out = [node, k, nsamples, desc_len]\n",
      "\n",
      "    # Full descriptor, output might be huge.\n",
      "    # by default is not stored. But can be enabled.\n",
      "    if self.options.store_mu:\n",
      "      out.append(mu.tolist())\n",
      "\n",
      "    if self.options.store_eigen_vectors:\n",
      "      out.append(D[:k].tolist())\n",
      "      out.append(U[:, :k].tolist())\n",
      "\n",
      "    yield None, out\n",
      "\n",
      "  def steps(self):\n",
      "    return [\n",
      "      self.mr(mapper_init=self.node_mapper_init, mapper=self.node_mapper,\n",
      "              reducer=self.node_substract_mean_reducer),\n",
      "      self.mr(reducer=self.node_descriptor_reducer)\n",
      "    ]\n",
      "\n",
      "  def configure_options(self):\n",
      "    super(MRPCA, self).configure_options()\n",
      "\n",
      "    self.add_passthrough_option(\n",
      "      '--no-debug',\n",
      "      default=None,\n",
      "      dest='no_debug',\n",
      "      action='store_true',\n",
      "      help='Don\\'t output debug message, default is false.')\n",
      "\n",
      "    self.add_passthrough_option(\n",
      "      '--reduced-dimension',\n",
      "      type='int',\n",
      "      dest='reduced_dimension',\n",
      "      default=None,\n",
      "      help='Reduced vector dimension (full is 730) for debugging. If it is not'\n",
      "      'set, use full dimension. Default is not set.')\n",
      "\n",
      "    self.add_passthrough_option(\n",
      "      '--explained-variance-ratio-threshold',\n",
      "      type='float',\n",
      "      dest='explained_variance_ratio_threshold',\n",
      "      default=0.99,\n",
      "      help='The threshold of explaine variance ration used in PCA process. Default is 0.99.')\n",
      "\n",
      "    self.add_passthrough_option(\n",
      "      '--num-levels',\n",
      "      type='int',\n",
      "      dest='num_levels',\n",
      "      default=9,\n",
      "      help='Number of total partition levels. Default is 9.')\n",
      "\n",
      "    self.add_passthrough_option(\n",
      "      '--store-mu',\n",
      "      default=None,\n",
      "      dest='store_mu',\n",
      "      action='store_true',\n",
      "      help='Store mean vector. By default mean vector is not stored')\n",
      "\n",
      "    self.add_passthrough_option(\n",
      "      '--store-eigen-vectors',\n",
      "      default=None,\n",
      "      dest='store_eigen_vectors',\n",
      "      action='store_true',\n",
      "      help='Store eigen vectors. By default eigne vectors is not stored.')\n",
      "\n",
      "    self.add_file_option(\n",
      "      '--station-to-node',\n",
      "      default='data/station-to-node.csv',\n",
      "      help='The csv file describes the mapping between station-id to node-id.')\n",
      "\n",
      "if __name__ == '__main__':\n",
      "  MRPCA.run()\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#### python mr_weather_pca.py --help\n",
      "[back to top](#top-5) <a name='mr_weather_pca_usage'></a>"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "!python mr_weather_pca.py --help"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Usage: mr_weather_pca.py [options] [input files]\r\n",
        "\r\n",
        "Options:\r\n",
        "  --help                show this message and exit\r\n",
        "  --help-emr            show EMR-related options\r\n",
        "  --help-hadoop         show Hadoop-related options\r\n",
        "  --help-runner         show runner-related options\r\n",
        "  --no-debug            Don't output debug message, default is false.\r\n",
        "  --reduced-dimension=REDUCED_DIMENSION\r\n",
        "                        Reduced vector dimension (full is 730) for debugging. If it is notset, use\r\n",
        "                        full dimension. Default is not set.\r\n",
        "  --explained-variance-ratio-threshold=EXPLAINED_VARIANCE_RATIO_THRESHOLD\r\n",
        "                        The threshold of explaine variance ration used in PCA process. Default is\r\n",
        "                        0.99.\r\n",
        "  --num-levels=NUM_LEVELS\r\n",
        "                        Number of total partition levels. Default is 9.\r\n",
        "  --store-mu            Store mean vector. By default mean vector is not stored\r\n",
        "  --store-eigen-vectors\r\n",
        "                        Store eigen vectors. By default eigne vectors is not stored.\r\n",
        "  --station-to-node=STATION_TO_NODE\r\n",
        "                        The csv file describes the mapping between station-id to node-id.\r\n",
        "\r\n",
        "  Running specific parts of the job:\r\n",
        "    --mapper            run a mapper\r\n",
        "    --combiner          run a combiner\r\n",
        "    --reducer           run a reducer\r\n",
        "    --step-num=STEP_NUM\r\n",
        "                        which step to execute (default is 0)\r\n",
        "    --steps             print the mappers, combiners, and reducers that this job defines\r\n",
        "\r\n",
        "  Protocols:\r\n",
        "    --strict-protocols  If something violates an input/output protocol then raise an exception\r\n"
       ]
      }
     ],
     "prompt_number": 15
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#### run_mr_weather_pca.py\n",
      "[back to top](#top-5) <a name='run_mr_weather_pca'></a>"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%load run_mr_weather_pca.sh"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 18
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#! /bin/sh\n",
      "JOB_FLOW_ID='j-XF54NKBXHHP0'\n",
      "\n",
      "MODE=$1\n",
      "if [[ $MODE == '' ]]; then\n",
      "  MODE=local\n",
      "fi\n",
      "\n",
      "if [[ $MODE == 'emr' ]]; then\n",
      "  python mr_weather_pca.py -r emr \\\n",
      "    --emr-job-flow-id ${JOB_FLOW_ID} \\\n",
      "    -c ~/.mrjob.conf \\\n",
      "    --explained-variance-ratio-threshold 0.99 \\\n",
      "    --station-to-node s3://weather-analysis/station-to-node-table-yoav.csv \\\n",
      "    --setup 'export PYTHONPATH=$PYTHONPATH:geo_partition.tar.gz#/' \\\n",
      "    --output-dir=s3://weather-analysis/node-descriptor-k-n-dl --no-output \\\n",
      "    --no-debug \\\n",
      "    s3://weather-analysis/weather-tminmax/\n",
      "elif [[ $MODE == 'emr-test' ]]; then\n",
      "  python mr_weather_pca.py -r emr \\\n",
      "    --emr-job-flow-id ${JOB_FLOW_ID} \\\n",
      "    -c ~/.mrjob.conf \\\n",
      "    --file s3://weather-analysis/station-to-node-table-yoav.csv \\\n",
      "    --setup 'export PYTHONPATH=$PYTHONPATH:geo_partition.tar.gz#/' \\\n",
      "    --output-dir=s3://weather-analysis/node-descriptor-k-n-dl --no-output \\\n",
      "    data/weather-tminmax-head-100.txt\n",
      "else\n",
      "  python mr_weather_pca.py \\\n",
      "    --reduced-dimension 5 \\\n",
      "    --explained-variance-ratio-threshold 0.99 \\\n",
      "    --store-mu \\\n",
      "    --store-eigen-vectors \\\n",
      "    --station-to-node data/station-to-node-table-yoav.csv \\\n",
      "    data/weather-tminmax-head-100.txt\n",
      "fi\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#### mr_weather_avg_temp.py\n",
      "[back to top](#top-avg-temp-regression) <a name='mr_weather_avg_temp'></a>"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%load mr_weather_avg_temp.py"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "\n",
      "from sys import stderr\n",
      "from mrjob.job import MRJob\n",
      "from mrjob.protocol import RawValueProtocol, JSONProtocol, PickleProtocol\n",
      "\n",
      "class MRWeatherAvgTemp(MRJob):\n",
      "  INPUT_PROTOCOL = JSONProtocol\n",
      "  INTERNAL_PROTOCOL = PickleProtocol\n",
      "  OUTPUT_PROTOCOL = RawValueProtocol\n",
      "\n",
      "  ERR = None\n",
      "  ERR_REPORTED = False\n",
      "\n",
      "  def debug(self, x):\n",
      "    stderr.write(str(x) + '\\n')\n",
      "\n",
      "  def parse_year(self, x):\n",
      "    try:\n",
      "      year = int(x)\n",
      "    except ValueError:\n",
      "      return None\n",
      "\n",
      "    if year < 0 or np.isnan(year): return None\n",
      "    return year\n",
      "\n",
      "  def mapper(self, key, tminmax_vec):\n",
      "\n",
      "    station, year = key.split(':')\n",
      "    year = self.parse_year(year)\n",
      "    avg_temp = np.mean(tminmax_vec)\n",
      "\n",
      "    if year is not None:\n",
      "      yield station, (year, avg_temp)\n",
      "      # try:\n",
      "      #   yield station, (year, avg_temp)\n",
      "      # except Exception, e:\n",
      "      #   yield 'error', str(e)\n",
      "\n",
      "  def reducer(self, station, vals):\n",
      "\n",
      "    x, y = [], []\n",
      "    for year, avg_temp in vals:\n",
      "      x.append(year)\n",
      "      y.append(avg_temp)\n",
      "\n",
      "    # Make sure the linear fit is meaningful:\n",
      "    if len(x) > 2:\n",
      "      A, b, r_v, p_v, std_err = linregress(x, y)\n",
      "\n",
      "      record = map(str, [station, A, b, r_v, p_v, std_err])\n",
      "      record = ','.join(record)\n",
      "\n",
      "      try:\n",
      "        yield None, record\n",
      "      except Exception, e:\n",
      "        yield 'error', str(e)\n",
      "\n",
      "    if MRWeatherAvgTemp.ERR is not None and MRWeatherAvgTemp.ERR_REPORTED == False:\n",
      "      yield 'error', str(MRWeatherAvgTemp.ERR)\n",
      "      MRWeatherAvgTemp.ERR_REPORTED = True\n",
      "\n",
      "if __name__ == '__main__':\n",
      "  try:\n",
      "    import numpy as np\n",
      "    from scipy.stats import linregress\n",
      "  except Exception, e:\n",
      "    MRWeatherAvgTemp.ERR = e\n",
      "\n",
      "  MRWeatherAvgTemp.run()\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#### partition_node_merge.py\n",
      "[back to top](#top-6) <a name='partition_node_merge'></a>"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%load partition_node_merge.py"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 3
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "class Node:\n",
      "  def __init__(self, id, k, nsamples, desc_len, coord='', thres=0.0, should_merge=False):\n",
      "    self.id = id\n",
      "    self.k = int(k)\n",
      "    self.nsamples = int(nsamples)\n",
      "    self.desc_len = int(desc_len)\n",
      "    self.coord = str(coord)\n",
      "    self.thres = float(thres)\n",
      "    self.should_merge = bool(should_merge);\n",
      "    self.group = 0;\n",
      "\n",
      "  def __repr__(self):\n",
      "    return self.to_csv_line()\n",
      "\n",
      "  def level(self):\n",
      "    return len(self.id)\n",
      "\n",
      "  def left_child(self, nodes_dict):\n",
      "    id = self.id + '0'\n",
      "    return nodes_dict[id] if nodes_dict.has_key(id) else None\n",
      "\n",
      "  def right_child(self, nodes_dict):\n",
      "    id = self.id + '1'\n",
      "    return nodes_dict[id] if nodes_dict.has_key(id) else None\n",
      "\n",
      "  def parent(self, nodes_dict):\n",
      "    id = self.id[:-1]\n",
      "    return nodes_dict[id] if nodes_dict.has_key(id) else None\n",
      "\n",
      "  def to_csv_line(self):\n",
      "    l = map(str, [\n",
      "      self.id,\n",
      "      self.coord,\n",
      "      self.thres,\n",
      "      self.k,\n",
      "      self.nsamples,\n",
      "      self.desc_len,\n",
      "      self.group\n",
      "      # '1' if self.should_merge else '0'\n",
      "    ])\n",
      "    return ','.join(l)\n",
      "\n",
      "def print_all_nodes(nodes_dict):\n",
      "  for k, v in nodes_dict.items():\n",
      "    print k, v\n",
      "\n",
      "def read_node_descriptors_from_csv(fname):\n",
      "  nodes_dict = {}\n",
      "  with open(fname) as f:\n",
      "    for line in f:\n",
      "      id, k, nsamples, desc_len = line.split(',')\n",
      "      nodes_dict[id] = Node(id, k, nsamples, desc_len)\n",
      "  return nodes_dict\n",
      "\n",
      "def join_node_spatial_info_from_csv(nodes_dict, fname):\n",
      "  with open(fname) as f:\n",
      "    for line in f:\n",
      "      id, coord, thres = line.split(',')\n",
      "      if nodes_dict.has_key(id):\n",
      "        nodes_dict[id].coord = coord\n",
      "        nodes_dict[id].thres = float(thres)\n",
      "  return nodes_dict\n",
      "\n",
      "def write_nodes_to_csv(nodes_dict, fname):\n",
      "  with open(fname, 'w') as f:\n",
      "    for _, n in nodes_dict.items():\n",
      "      f.write(n.to_csv_line())\n",
      "      f.write('\\n')\n",
      "\n",
      "def compute_whether_children_of_nodes_should_be_merged_at_level(nodes_dict, level):\n",
      "  nodes = [v for k, v in nodes_dict.items() if v.level() == level]\n",
      "\n",
      "  merge_count = 0\n",
      "  total_count = 0\n",
      "\n",
      "  for n in nodes:\n",
      "    total_count += 1\n",
      "    lc, rc = n.left_child(nodes_dict), n.right_child(nodes_dict)\n",
      "    if lc and rc and lc.desc_len + rc.desc_len > n.desc_len:\n",
      "      merge_count += 1\n",
      "      lc.should_merge = True\n",
      "      rc.should_merge = True\n",
      "  print level, merge_count, '/', total_count\n",
      "  return nodes_dict\n",
      "\n",
      "def compute_nodes_should_be_merged(nodes_dict, num_levels):\n",
      "  for i in range(num_levels + 1):\n",
      "    compute_whether_children_of_nodes_should_be_merged_at_level(nodes_dict, i)\n",
      "  return nodes_dict\n",
      "\n",
      "def compute_nodes_groups(nodes_dict):\n",
      "  nodes = [v for k, v in nodes_dict.items()]\n",
      "\n",
      "  for node in nodes:\n",
      "    nid = node.parent(nodes_dict).left_child(nodes_dict).id if node.should_merge else node.id\n",
      "    gid = int(nid, 2) + 1 if nid != '' else 0\n",
      "    node.group = gid\n",
      "\n",
      "\n",
      "if __name__ == '__main__':\n",
      "  fin_name_1 = 'data/node-descriptor-k-n-dl-1-of-100.csv'\n",
      "  fin_name_2 = 'data/partition-tree-yoav.csv'\n",
      "  fout_name = 'data/partition-tree-nid-coord-thres-k-n-dl-gid-1-of-100.csv'\n",
      "  num_levels = 9\n",
      "\n",
      "  nodes_dict = read_node_descriptors_from_csv(fin_name_1)\n",
      "  join_node_spatial_info_from_csv(nodes_dict, fin_name_2)\n",
      "  compute_nodes_should_be_merged(nodes_dict, num_levels)\n",
      "  compute_nodes_groups(nodes_dict)\n",
      "\n",
      "  write_nodes_to_csv(nodes_dict, fout_name)\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#### ks_test_merge.py\n",
      "[back to top](#top-merge-by-level-kstest) <a name='ks_test_merge'></a>"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%load ks_test_merge.py"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 10
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "\n",
      "from random import random\n",
      "import sys\n",
      "# TODO:\n",
      "# Input: vec_a, vec_b are two array of double of same length\n",
      "# Output: p value\n",
      "\n",
      "# Implement kstest using scipy.stats.kstest\n",
      "from scipy.stats import ks_2samp\n",
      "def kstest(vec_a, vec_b):\n",
      "  _, p = ks_2samp(vec_a, vec_b)\n",
      "  return p\n",
      "\n",
      "\n",
      "class Node:\n",
      "  def __init__(self, id, tmin_vec, tmax_vec):\n",
      "    self.id = id\n",
      "    self.tmin_vec = tmin_vec\n",
      "    self.tmax_vec = tmax_vec\n",
      "\n",
      "  def get_sibling_id(self):\n",
      "    parent_id = self.id[:-1]\n",
      "    last_bit = self.id[-1]\n",
      "    if last_bit == '0':\n",
      "      return parent_id + '1'\n",
      "    else:\n",
      "      return parent_id + '0'\n",
      "\n",
      "def get_all_left_nodes_id(nodes_dict):\n",
      "  left_nodes = []\n",
      "  for nid in nodes_dict:\n",
      "    if len(nid) > 0:\n",
      "      last_bit = nid[-1]\n",
      "      if last_bit == '0':\n",
      "        left_nodes.append(nid)\n",
      "  return left_nodes\n",
      "\n",
      "def read_nodes_from_csv(in_file):\n",
      "  nodes_dict = {}\n",
      "  with open(in_file, 'r') as f:\n",
      "    for line in f:\n",
      "      items = line.strip().split(',')\n",
      "\n",
      "      node_id = items[0]\n",
      "      len_tmin_vec = int(items[1])\n",
      "      len_tmax_vec = int(items[2])\n",
      "\n",
      "      tmin_vec, tmax_vec = [], []\n",
      "\n",
      "      for i in range(len_tmin_vec):\n",
      "        tmin_vec.append(float(items[i + 3]))\n",
      "\n",
      "      for i in range(len_tmax_vec):\n",
      "        tmax_vec.append(float(items[i + 3 + len_tmin_vec]))\n",
      "\n",
      "      nodes_dict[node_id] = Node(node_id, tmin_vec, tmax_vec)\n",
      "\n",
      "    return nodes_dict\n",
      "\n",
      "def compute_node_pairs_should_merge(nodes, thres=0.05):\n",
      "  tmin_pairs, tmax_pairs = [], []\n",
      "\n",
      "  left_nodes = get_all_left_nodes_id(nodes)\n",
      "\n",
      "  for left_node_id in left_nodes:\n",
      "    left_node = nodes[left_node_id]\n",
      "    right_node_id = left_node.get_sibling_id()\n",
      "    right_node = nodes[right_node_id]\n",
      "\n",
      "    # base on kstest on tmin_vec\n",
      "    a = left_node.tmin_vec\n",
      "    b = right_node.tmin_vec\n",
      "    p = kstest(a, b)\n",
      "    if p > thres:\n",
      "      tmin_pairs.append((left_node.id, right_node.id))\n",
      "\n",
      "    # base on kstest on tmax_vec\n",
      "    a = left_node.tmax_vec\n",
      "    b = right_node.tmax_vec\n",
      "    p = kstest(a, b)\n",
      "    if p > thres:\n",
      "      tmax_pairs.append((left_node.id, right_node.id))\n",
      "\n",
      "  return tmin_pairs, tmax_pairs\n",
      "\n",
      "def output_node_pairs_should_merge(pairs, f):\n",
      "  for p in pairs:\n",
      "    f.write(p[0] + ',' + p[1] + '\\n')\n",
      "\n",
      "if __name__ == '__main__':\n",
      "  in_file = '/home/GL/Desktop/node-daily-avg-tminmax.csv'\n",
      "  out_file_1 = 'data/node-pairs-should-merge-using-kstest-tmin.csv'\n",
      "  out_file_2 = 'data/node-pairs-should-merge-using-kstest-tmax.csv'\n",
      "\n",
      "  nodes = read_nodes_from_csv(in_file)\n",
      "  tmin_pairs, tmax_pairs = compute_node_pairs_should_merge(nodes)\n",
      "\n",
      "  # output_node_pairs_should_merge(tmin_pairs, sys.stdout)\n",
      "\n",
      "  with open(out_file_1, 'w') as f1, open(out_file_2, 'w') as f2:\n",
      "    output_node_pairs_should_merge(tmin_pairs, f1)\n",
      "    output_node_pairs_should_merge(tmax_pairs, f2)\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}